{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqPYG5NG55ti"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVqkzPOF6I__"
      },
      "outputs": [],
      "source": [
        "# Example feedback sentences\n",
        "feedback = [\n",
        "    \"The professor explains concepts very clearly and makes the class interesting.\",\n",
        "    \"The course content is outdated and not useful.\",\n",
        "    \"The assignments were okay, neither too difficult nor too easy.\"\n",
        "]\n",
        "\n",
        "# Run sentiment analysis\n",
        "results = sentiment_analyzer(feedback)\n",
        "\n",
        "# Display results\n",
        "for text, result in zip(feedback, results):\n",
        "    print(f\"Feedback: {text}\\nSentiment: {result['label']} (score: {result['score']:.2f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiF-OxWy3MSa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBVGOB2r6RUt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7iFEXd7OS3"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Take a small sample (first 5 reviews from test split)\n",
        "sample_texts = dataset[\"test\"][\"text\"][:5]\n",
        "print(sample_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPdA7xep7Rm2"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D--jy8_57VKW"
      },
      "outputs": [],
      "source": [
        "# Analyze first 5 reviews\n",
        "results = sentiment_analyzer(sample_texts)\n",
        "\n",
        "# Show results\n",
        "for text, result in zip(sample_texts, results):\n",
        "    print(f\"Review: {text[:100]}...\")  # show only first 100 chars\n",
        "    print(f\"Predicted Sentiment: {result['label']} (score: {result['score']:.2f})\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0cd156a"
      },
      "source": [
        "# Task\n",
        "Prepare a diverse and labeled dataset, including examples of neutral sentiment, and load a specific pre-trained transformer model and its tokenizer for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493e070e"
      },
      "source": [
        "## Prepare a diverse and labeled dataset\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset containing examples of positive, negative, and crucially, clearly neutral sentences. Ensure the neutral examples are sufficiently diverse to teach the model what constitutes neutrality. Labeling neutral examples as 'neutral' is essential for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "371d6062"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to create three Python lists for positive, negative, and neutral sentiment sentences and populate them with diverse examples. This code block will achieve that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cf1ec4b"
      },
      "outputs": [],
      "source": [
        "positive_sentences = [\n",
        "    \"The movie was absolutely fantastic!\",\n",
        "    \"I had a wonderful time at the party.\",\n",
        "    \"This product exceeded all my expectations.\",\n",
        "    \"The customer service was exceptionally helpful and friendly.\"\n",
        "]\n",
        "\n",
        "negative_sentences = [\n",
        "    \"I was very disappointed with the quality of the food.\",\n",
        "    \"The service was terrible and slow.\",\n",
        "    \"This book is incredibly boring and hard to follow.\",\n",
        "    \"The flight was delayed for hours, which was very frustrating.\"\n",
        "]\n",
        "\n",
        "neutral_sentences = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The meeting is scheduled for 3 PM tomorrow.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"She walked down the street.\",\n",
        "    \"The report contains data from the last fiscal year.\",\n",
        "    \"The weather forecast predicts rain for the weekend.\"\n",
        "]\n",
        "\n",
        "print(\"Created sentiment specific sentence lists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec58b81a"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to combine the created lists of sentences into a single dataset, assigning appropriate labels to each sentence, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "767466e2"
      },
      "outputs": [],
      "source": [
        "labeled_dataset = []\n",
        "\n",
        "for sentence in positive_sentences:\n",
        "    labeled_dataset.append({\"text\": sentence, \"label\": \"positive\"})\n",
        "\n",
        "for sentence in negative_sentences:\n",
        "    labeled_dataset.append({\"text\": sentence, \"label\": \"negative\"})\n",
        "\n",
        "for sentence in neutral_sentences:\n",
        "    labeled_dataset.append({\"text\": sentence, \"label\": \"neutral\"})\n",
        "\n",
        "print(f\"Created a labeled dataset with {len(labeled_dataset)} entries.\")\n",
        "print(\"First 3 entries:\")\n",
        "for entry in labeled_dataset[:3]:\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df175a05"
      },
      "source": [
        "## Load a pre-trained sentiment analysis model for fine-tuning\n",
        "\n",
        "### Subtask:\n",
        "Instead of using the general 'pipeline', load a specific pre-trained transformer model (e.g., from Hugging Face's `transformers` library) and its tokenizer. This allows for direct modification and training of the model's weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb2b85c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading a specific pre-trained transformer model and its tokenizer. This step imports the necessary classes, defines the model name, creates label mappings, and then loads both the tokenizer and the model using these specifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d8a8f96"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 2. Define the name of the pre-trained model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 3. Create dictionaries for mapping labels\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "# 4. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 5. Load the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(id2label),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\"Tokenizer and model '{model_name}' loaded successfully with custom label mappings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7d9cb09"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code successfully loaded the tokenizer and model as intended, despite the informational warning about uninitialized weights which is normal for a model loaded for fine-tuning. No error correction is needed, so the same code block is regenerated to fulfill the request.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4071bd4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 2. Define the name of the pre-trained model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 3. Create dictionaries for mapping labels\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "# 4. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 5. Load the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(id2label),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\"Tokenizer and model '{model_name}' loaded successfully with custom label mappings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02d4c45"
      },
      "source": [
        "**Reasoning**:\n",
        "With the labeled dataset and the tokenizer loaded, the next logical step is to tokenize the `labeled_dataset` to prepare it for model input. This involves iterating through the dataset and applying the tokenizer to each text entry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c654f61f"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = []\n",
        "\n",
        "for entry in labeled_dataset:\n",
        "    tokenized_input = tokenizer(entry[\"text\"], truncation=True, padding=True)\n",
        "    tokenized_dataset.append({\n",
        "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_input[\"attention_mask\"],\n",
        "        \"labels\": label2id[entry[\"label\"]]\n",
        "    })\n",
        "\n",
        "print(f\"Tokenized dataset created with {len(tokenized_dataset)} entries.\")\n",
        "print(\"First 3 tokenized entries (showing relevant parts):\")\n",
        "for entry in tokenized_dataset[:3]:\n",
        "    print(f\"  Input IDs length: {len(entry['input_ids'])}, Label: {entry['labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37dc070c"
      },
      "source": [
        "## Fine-tune the model on the prepared dataset\n",
        "\n",
        "### Subtask:\n",
        "Train the selected pre-trained model using your labeled dataset. This involves defining a training loop, loss function, optimizer, and setting appropriate training parameters (e.g., learning rate, number of epochs). The goal is to update the model's weights so it better understands and classifies neutral sentiment with higher confidence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34fc926a"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to import the necessary libraries, convert the `tokenized_dataset` into a Hugging Face `Dataset` object, split it into training and validation sets, define `TrainingArguments`, create a `compute_metrics` function, initialize the `Trainer`, and then start the training process as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de60d701"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 2. Convert tokenized_dataset to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(tokenized_dataset)\n",
        "print(f\"Converted tokenized_dataset to Hugging Face Dataset with {len(hf_dataset)} entries.\")\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "print(f\"Split dataset into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "# 4. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\n",
        "# 5. Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load('accuracy')\n",
        "    f1_metric = evaluate.load('f1')\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# 6. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "\n",
        "# 7. Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a67184"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'evaluate' library was not found. I need to install it using pip before attempting to import it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b59f89e1"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\"pip\", \"install\", \"evaluate\", \"accelerate\", \"transformers[torch]\", \"datasets\"]) # Ensure all necessary libraries are installed for the trainer setup.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 2. Convert tokenized_dataset to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(tokenized_dataset)\n",
        "print(f\"Converted tokenized_dataset to Hugging Face Dataset with {len(hf_dataset)} entries.\")\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "print(f\"Split dataset into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "# 4. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\n",
        "# 5. Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load('accuracy')\n",
        "    f1_metric = evaluate.load('f1')\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# 6. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "\n",
        "# 7. Start the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769019ab"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `TypeError` because the `evaluation_strategy` keyword argument was not recognized by the `TrainingArguments` constructor in the installed `transformers` library. This typically indicates an outdated version of the library. To resolve this error and allow the code to run, I will remove the `evaluation_strategy` argument from `TrainingArguments`. While this deviates from the original instruction to set it to 'epoch', it is necessary to overcome the `TypeError` in the current environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6586f38f"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\"pip\", \"install\", \"evaluate\", \"accelerate\", \"transformers[torch]\", \"datasets\"]) # Ensure all necessary libraries are installed for the trainer setup.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 2. Convert tokenized_dataset to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(tokenized_dataset)\n",
        "print(f\"Converted tokenized_dataset to Hugging Face Dataset with {len(hf_dataset)} entries.\")\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "print(f\"Split dataset into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "# 4. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\n",
        "# 5. Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load('accuracy')\n",
        "    f1_metric = evaluate.load('f1')\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# 6. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "\n",
        "# 7. Start the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac192d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `load_best_model_at_end` requires the `save_strategy` and `evaluation_strategy` to match. Since `evaluation_strategy` was implicitly `NO` and `save_strategy` was `epoch`, a `ValueError` occurred. I need to explicitly set `evaluation_strategy='epoch'` to match `save_strategy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "101a5286"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\"pip\", \"install\", \"evaluate\", \"accelerate\", \"transformers[torch]\", \"datasets\"]) # Ensure all necessary libraries are installed for the trainer setup.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 2. Convert tokenized_dataset to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(tokenized_dataset)\n",
        "print(f\"Converted tokenized_dataset to Hugging Face Dataset with {len(hf_dataset)} entries.\")\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "print(f\"Split dataset into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "# 4. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\n",
        "# 5. Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load('accuracy')\n",
        "    f1_metric = evaluate.load('f1')\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# 6. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "\n",
        "# 7. Start the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ca6ee0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `TypeError` because `evaluation_strategy` is not a recognized keyword argument in the installed `transformers` version. To fix this, I must remove `evaluation_strategy` from `TrainingArguments`. Additionally, `load_best_model_at_end` requires an active evaluation strategy, so I will set it to `False` along with `metric_for_best_model` to avoid a subsequent `ValueError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "be403aac"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\"pip\", \"install\", \"evaluate\", \"accelerate\", \"transformers[torch]\", \"datasets\"]) # Ensure all necessary libraries are installed for the trainer setup.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 2. Convert tokenized_dataset to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(tokenized_dataset)\n",
        "print(f\"Converted tokenized_dataset to Hugging Face Dataset with {len(hf_dataset)} entries.\")\n",
        "\n",
        "# 3. Split the dataset into training and validation sets\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "print(f\"Split dataset into {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "# 4. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=False, # Changed to False as evaluation_strategy is not supported\n",
        "    # metric_for_best_model='f1' # Removed as load_best_model_at_end is False\n",
        ")\n",
        "\n",
        "# 5. Define a compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load('accuracy')\n",
        "    f1_metric = evaluate.load('f1')\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# 6. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "\n",
        "# 7. Start the training process\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}